{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Generic Diction in Latin Poetry with Scattertext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook plots keyness in generic diction in Latin poetry, specifically Latin love elegy (Propertius, Tibullus, early Ovid) and epic (Virgil's *Aeneid*) and it does so using the very attractive interactive plots produced by Jason Kessler's [Scattertext](https://github.com/JasonKessler/scattertext). Kessler's Scattertext demo plot visualized the relative keyness of Democratic/Republican diction using the corpus of 2012 political convention addresses. I have created a binary model of my own, following my work on elegiac diction in Latin epic poetry from my dissertation [\"*Amor belli*: Elegiac Diction and the Theme of Love in Lucan's *Bellum civile*\"](https://fordham.bepress.com/dissertations/AAI10125245/) as well as a related article in the *Journal of Data Mining and Digital Humanities* titled [\"Measuring and Mapping Intergeneric Allusion using *Tesserae*\"](https://hal.archives-ouvertes.fr/hal-01282568). Here I create a chunked corpus from the authors listed above. The texts are preprocessed, lemmatized, and then plotted using Scattertext. As I mentioned above, the results are attractive.\n",
    "\n",
    "![Scattertext visualization of elegy/epic](./images/scattertext.png \"Scattertext visualization of elegy/epic\")\n",
    "\n",
    "In a single, easy-to-take-in plot, we get an excellent sense of which words are 'elegiac' and which words are 'epic' as well as their relative frequency. (The fact that we can easily search for terms in context is a nice bonus, as is the summary information.) Readers of Latin poetry should not be suprised by the upper-left and lower-right corners, that is frequent-elegiac and frequent-epic, respectively. *Puella*, *domina*, *formosus* are decidely elegiac; *clipeus*, *arduus*, *immanis* are epic. Cynthia finds a place among the elegiac words, while Turnus and Anchises rest in epic diction. (Aeneas is more epic than not, but [*Heroides* 7](http://www.thelatinlibrary.com/ovid/ovid.her7.shtml) alone brings up the elegiac value.) But what is much more interesting to me, and what has been a central idea in my research on generic diction are the words in the middle of the plot. \n",
    "\n",
    "This plot makes clear that generic diction does not have to be thought of in discrete, binary terms, but rather we can create language models like the simplified elegy-epic model presented here, that show generic diction as continuous. For example, I can say not only that *cupio* is more elegiac than epic, but I can be more specific and say that—in this model at least—*cupio* is 79% elegiac / 21% epic. In future notebooks, I will expand on this kind of thinking, that is observing literary features as continuous variables in texts. For example, by assigning generic weights to words in these texts we can map epicness or elegiacness in [narrative space](https://github.com/diyclassics/literature-experiments/blob/master/plot-narrative-space.ipynb).\n",
    "\n",
    "One last point—this notebook uses [spaCy's English NLP tools](https://spacy.io) for preparing the Scattertext plots. I have been working on developing [Latin-specific tools for spaCy](https://github.com/diyclassics/spaCy/tree/latin/spacy/lang) and this experiment further convinces me of the usefulness of this work. Hopefully this is something I can devote more time to soon. [PJB 3.22.18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e2e9b825dcaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatin\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlatinlibrary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "\n",
    "from cltk.corpus.latin import latinlibrary\n",
    "from cltk.tokenize.word import WordTokenizer\n",
    "from cltk.stem.latin.j_v import JVReplacer\n",
    "from cltk.lemmatize.latin.backoff import BackoffLatinLemmatizer\n",
    "from cltk.utils.file_operations import open_pickle\n",
    "\n",
    "import scattertext as st\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up spaCy for Scattertext\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to import a data model to train the lemmatizer.\n",
    "\n",
    "# Set up training sentences\n",
    "rel_path = os.path.join('~/cltk_data/latin/model/latin_models_cltk/lemmata/backoff')\n",
    "path = os.path.expanduser(rel_path)\n",
    "\n",
    "# Check for presence of latin_pos_lemmatized_sents\n",
    "file = 'latin_pos_lemmatized_sents.pickle'      \n",
    "\n",
    "latin_pos_lemmatized_sents_path = os.path.join(path, file)\n",
    "if os.path.isfile(latin_pos_lemmatized_sents_path):\n",
    "    latin_pos_lemmatized_sents = open_pickle(latin_pos_lemmatized_sents_path)\n",
    "else:\n",
    "    latin_pos_lemmatized_sents = []\n",
    "    print('The file %s is not available in cltk_data' % file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up CLTK tools\n",
    "\n",
    "word_tokenizer = WordTokenizer('latin')\n",
    "replacer = JVReplacer()\n",
    "lemmatizer = BackoffLatinLemmatizer(latin_pos_lemmatized_sents)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Latin Library files and build text array\n",
    "\n",
    "virgil = [file for file in latinlibrary.fileids() if \"vergil/a\" in file]\n",
    "propertius = ['prop2.txt', 'prop3.txt', 'prop4.txt', 'propertius1.txt']\n",
    "tibullus = ['tibullus1.txt', 'tibullus2.txt']\n",
    "ovid = [file for file in latinlibrary.fileids() if \"ovid.amor\" in file]\n",
    "\n",
    "TextArray = [\n",
    "    ('epic', 'aeneid 1', 'vergil/aen1.txt'),\n",
    "    ('epic', 'aeneid 2', 'vergil/aen2.txt'),\n",
    "    ('epic', 'aeneid 3', 'vergil/aen3.txt'),\n",
    "    ('epic', 'aeneid 4', 'vergil/aen4.txt'),\n",
    "    ('epic', 'aeneid 5', 'vergil/aen5.txt'),\n",
    "    ('epic', 'aeneid 6', 'vergil/aen6.txt'),\n",
    "    ('epic', 'aeneid 7', 'vergil/aen7.txt'),\n",
    "    ('epic', 'aeneid 8', 'vergil/aen8.txt'),\n",
    "    ('epic', 'aeneid 9', 'vergil/aen9.txt'),\n",
    "    ('epic', 'aeneid 10', 'vergil/aen10.txt'),\n",
    "    ('epic', 'aeneid 11', 'vergil/aen11.txt'),\n",
    "    ('epic', 'aeneid 12', 'vergil/aen12.txt'),\n",
    "    ('elegy', 'propertius 1', 'propertius1.txt'),\n",
    "    ('elegy', 'propertius 2', 'prop2.txt'),\n",
    "    ('elegy', 'propertius 3', 'prop3.txt'),\n",
    "    ('elegy', 'propertius 4', 'prop4.txt'),\n",
    "    ('elegy', 'tibullus 1', 'tibullus1.txt'),\n",
    "    ('elegy', 'tibullus 2', 'tibullus2.txt'),\n",
    "    ('elegy', 'amores 1', 'ovid/ovid.amor1.txt'),\n",
    "    ('elegy', 'amores 2', 'ovid/ovid.amor2.txt'),\n",
    "    ('elegy', 'amores 3', 'ovid/ovid.amor3.txt'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script for preprocessing texts\n",
    "\n",
    "def preprocess(text):\n",
    "    \n",
    "    import html\n",
    "    import re\n",
    "    \n",
    "    text = html.unescape(text) # Handle html entities\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = replacer.replace(text) #Normalize u/v & i/j\n",
    "    \n",
    "    punctuation =\"\\\"#$%&\\'()*+,-/:;<=>@[\\]^_`{|}~.?!«»—\"\n",
    "    translator = str.maketrans({key: \" \" for key in punctuation})\n",
    "    text = text.translate(translator)\n",
    "    \n",
    "    translator = str.maketrans({key: \" \" for key in '0123456789'})\n",
    "    text = text.translate(translator)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Script for getting & preprocessing LL texts\n",
    "\n",
    "def get_ll_text(fileid):\n",
    "    text = latinlibrary.raw(fileid)\n",
    "    text = preprocess(text)\n",
    "    text = text[95:-95] # Fix to get real start of texts!\n",
    "    text = text[text.find(' '):]\n",
    "    return text\n",
    "\n",
    "# Script for chunking text\n",
    "\n",
    "def make_text_chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build chunked TextArray\n",
    "\n",
    "TextArrayChunks = []\n",
    "\n",
    "for item in TextArray:\n",
    "    genre, work, fileid = item\n",
    "    tokens = get_ll_text(fileid).split() \n",
    "    chunk_text = make_text_chunks(tokens, 250)\n",
    "    chunk_text = [\" \".join(chunk) for chunk in chunk_text]\n",
    "    for i, chunk in enumerate(list(chunk_text)):\n",
    "        chunk_name = '{}_{}'.format(work, i)\n",
    "        TextArrayChunks.append((genre, work, chunk_name, fileid, chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and populate text dataframe \n",
    "\n",
    "columns = ['genre', 'work', 'chunk_name', 'fileid', 'text']\n",
    "df = pd.DataFrame(TextArrayChunks, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example from dataframe\n",
    "\n",
    "df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example from dataframe\n",
    "\n",
    "df[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Scatterplot corpus\n",
    "\n",
    "corpus = st.CorpusFromPandas(df,\n",
    "                            category_col='genre',\n",
    "                            text_col='text',\n",
    "                            nlp=nlp).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give keyness example\n",
    "\n",
    "print(list(corpus.get_scaled_f_scores_vs_background().index[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Scattertext HTML file\n",
    "\n",
    "html_doc = st.produce_scattertext_explorer(corpus,\n",
    "          category='elegy',\n",
    "          category_name='Elegy',\n",
    "          not_category_name='Epic',\n",
    "          width_in_pixels=1000,\n",
    "          metadata=df['work'])\n",
    "\n",
    "open(\"Genre-Visualization.html\", 'wb').write(html_doc.encode('utf-8'))          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script for lemmatizing text\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    tokens = word_tokenizer.tokenize(text)\n",
    "    lemma_pairs = lemmatizer.lemmatize(tokens)\n",
    "    lemmas = [lemma[1] for lemma in lemma_pairs]\n",
    "    lemmatized_text = \" \".join(lemmas)\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize text column\n",
    "\n",
    "df['text'] = df.apply(lambda row: lemmatize_text(row['text']), axis=1)\n",
    "df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Scatterplot corpus with lemmatized texts\n",
    "\n",
    "corpus = st.CorpusFromPandas(df,\n",
    "                            category_col='genre',\n",
    "                            text_col='text',\n",
    "                            nlp=nlp).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get words with elegiac keyness\n",
    "\n",
    "term_freq_df = corpus.get_term_freq_df()\n",
    "term_freq_df['Elegiac Score'] = corpus.get_scaled_f_scores('elegy')\n",
    "pprint(list(term_freq_df.sort_values(by='Elegiac Score', ascending=False).index[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get words with epic keyness\n",
    "\n",
    "term_freq_df = corpus.get_term_freq_df()\n",
    "term_freq_df['Epic Score'] = corpus.get_scaled_f_scores('epic')\n",
    "pprint(list(term_freq_df.sort_values(by='Epic Score', ascending=False).index[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Scattertext HTML file with lemmatized text\n",
    "\n",
    "html_doc = st.produce_scattertext_explorer(corpus,\n",
    "          category='elegy',\n",
    "          category_name='Elegy',\n",
    "          not_category_name='Epic',\n",
    "          width_in_pixels=1000,\n",
    "          metadata=df['chunk_name'])\n",
    "\n",
    "open(\"Lemmatized-Genre-Visualization.html\", 'wb').write(html_doc.encode('utf-8'))        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
